# Gotutiyanの論文メモ

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['\$','\$'],['\\(','\\)']],processEscapes:true},CommonHTML: {matchFontHeight:false}});</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

# [GECToR – Grammatical Error Correction: Tag, Not Rewrite](https://aclanthology.org/2020.bea-1.16) (Omelianchuk et al., BEA 2020)

- Grammarlyのチーム
- 系列タグ付け（トークン単位の分類器）で教師あり設定の文法誤り訂正
- 入力：誤りを含む可能性のある文
- 出力：編集情報を示すタグ（実際にはその後タグを入力に反映し，訂正文を得るが，それはあくまで後処理）．
- タグは次の2種類
  - basic-transformation: 置換・挿入・削除・無編集の4種類に基づく．
  - g-transformation: 言語情報を考慮．「名詞を単数形に」とか，「動詞には過去形に」とか．
- 学習時は，タグの推定と単語レベルの誤り検出をマルチタスクで解く．
- 推論時は，複数サイクル訂正することで複数単語の挿入などの訂正を実現．訂正にはタグのみが使われ，サイクルをどこまで回すかに誤り検出の情報が使われる．
- 実験
  - 学習時には3段階の訓練．
  - 推論時は2回のサイクルを回せば大半の訂正は処理できる
  - エンコーダはRoBERTaとかXLNetを使っておけば良さそう
  - アンサンブルは効果あり，ベンチマークで2ポイント程度向上
- 感想：分類器のクラスを考えると，訂正しないときのタグは一つに集約されるが，訂正のタグは様々にあるのでラベルはすごく不均衡．でも誤り検出タスクでは訂正側のラベルも一つに集約されているとみなせるので，誤り検出を学習しているのが上手くいくキモ？

# [Language-agnostic Representation from Multilingual Sentence Encoders for Cross-lingual Similarity Estimation](https://aclanthology.org/2021.emnlp-main.612) (Tiyajamorn et al., EMNLP 2021)

- mBERTなどによる多言語文埋め込みを，意味情報の埋め込みと言語情報の埋め込みに分離する方法を提案．異なる言語間で意味類似性を正確に計算することが動機．

- 提案法では，翻訳のデータが（i）文対は異なる言語・同じ意味，（ii）原言語側もしくは目的言語側のみでは同じ言語・異なる意味，である性質を利用．

- 既存の文埋め込みを2種類の線形層に独立に通して，それぞれの出力が意味情報と言語情報を表すのだと思うことにする：

  $e = \mathrm{Encoder}(x)$

  $\hat{e}_M = \mathrm{MLP}_M(e)$

  $\hat{e}_L = \mathrm{MLP}_L(e)$．

- 学習時の損失は3種類．

  - Reconstruction loss: 両者を足せば元の埋め込みに戻る．
  - Language loss: 言語情報の埋め込みについて，同じ言語で異なる文同士の類似度が高くなるようにする．また，言語判定ができるようにする．
  - Meaning loss: 意味情報の埋め込みについて，文対間では類似度が高くなり，同じ言語で異なる文同士の類似度は低くなるようにする．

- 推論時に意味類似性を計算するときには，意味情報の空間で単にコサイン類似度を考えればいい．

- 実験では翻訳の品質推定タスク（WMT20 QE）と多言語の意味類似性を測定するタスク（SemEval-2017 Cross-lingual STS）を実施．多くの場合で，意味と言語を分離したほうが意味類似性を正確に計算できた．

- 感想：実装が楽だし，直感的にも上手くいきそうな手法に感じた．意味情報を正確に獲得するために言語情報を別の空間に追い出しているようなイメージなので，他のタスクでも埋め込みから"追い出したいもの"があれば使えそう．

# [Adjusting the Precision-Recall Trade-Off with Align-and-Predict Decoding for Grammatical Error Correction](https://aclanthology.org/2022.acl-short.77) (Sun & Wang, ACL 2022)

- 自己回帰モデルの推論時に，どれくらい書き換えを促進するかを制御する手法を提案．
- 提案法では，生成の途中で逐次的に入力文とのアライメントを取る．アライメントを取れば，現段階で入力文に対応する出力がどこまで生成できているかが分かるため，次にどの単語を生成すれば入力文のコピーになるかも分かる．コピーになる単語の生成確率を意図的に上げる・下げることで，積極的な書き換えを制御・促進する．
- ※アライメントとは言うが，生成途中系列の末尾部分についてわかればいいので，アライメントを全て求める必要はない．実装上は，keyにn-gram，valueにそのn-gramの次のトークンを格納する辞書を事前に構築することで高速に計算．
- ※論文の式4はlogの確率になっているため，式5で導入されている重み $\lambda$ の解釈に注意．
- 実験では英語と中国語の文法誤り訂正を行い， $\lambda$ の値によって訂正の積極性が制御されていることを確認．これはPrecisionとRecallを制御可能ということでもある． $\lambda=0.5$ 付近が良い感じらしい．
- 感想：推論時の手法なので，導入コストが低いのが良い．また任意の自己回帰モデルに利用できる．書き換えの積極性と訂正文の流暢性の関係は気になる．

# [Grammatical Error Correction: Are We There Yet?](https://aclanthology.org/2022.coling-1.246) (Qorib & Ng, COLING 2022)

- 現状の文法誤り訂正システムに存在する課題を分析した論文．
- 分析の対象のモデルは，T5ベース（自己回帰）とGECToR（非自己回帰）
- 以下の観点で課題があるとしている．
  - 不自然な表現の訂正．既存の訓練データは最小限に訂正されていることが主な原因．
  - 文構造の認識．主語や目的語が複合語句になっているときに失敗しがち．
  - 文の理解．書き手の意図を反映した訂正は難しい．
  - 誤り率が高い文の訂正．
  - 長い文の訂正．
  - 文脈を考慮した訂正．多くのモデルは単文の訂正にとどまる．
  - 異なるドメインへの適応．
- 以上の観察から，以上の項目を明示的に含んだような評価データが必要であると主張．
- 感想：文法誤り訂正分野の課題を実例も交えて真正面から指摘した点が貢献．論文では評価データの重要性のみ言及されていたが，解けないことを測定すると同時に解けるようにすることも必要なので，訓練データの整備も考えないとな，という気持ちに．

# [BLEURT: Learning Robust Metrics for Text Generation](https://aclanthology.org/2020.acl-main.704) (Sellam et al., ACL 2020)

- ニューラル（BERT）ベースのロバストな言語生成の評価尺度を作成．
- システム出力は日々変化するため，古いデータで学習された評価器はそれ以降のシステム出力を正しく評価できない可能性がある．
  - 重要なのはドメインや品質の観点で外挿が可能であること．
- 提案法では，ロバストな評価器を構築するためにWikipediaを元データとする擬似データによる訓練を実施．まず，マスクトークンの復元・逆翻訳・単語の削除により擬似的なシステム出力を作成する．この擬似出力文と参照文（元データ）からBLEUやBERTScoreなどの多様な評価尺度で評価値を計算しておく．事前訓練ではそれらの値を直接予測できるように評価器を訓練する．
- WMT17,18,19の評価タスクで実験．多くの場合で提案法が既存手法よりも人手評価との高い相関を達成．つまり，訓練よりも評価データが新しいものになってもロバストに評価可能．
- また，WMT17の評価タスクにおいて，品質が悪いものを訓練に，品質が高いものを評価に意図的におくような設定で実験．擬似データによる事前訓練をしたほうが，人手評価との高い相関を達成．つまり，事前訓練を行うことで，訓練と評価の品質が大きく異なってもロバストに評価可能．
- WebNLG Challenge 2017のタスクの評価についても実験．WebNLGのデータを使わずとも既存手法を上回る結果に．つまり，訓練と評価のドメインが異なっていてもロバストに評価可能．
- 事前訓練では多様な尺度を目的関数に事前訓練するが，BLEUとROUGEは悪影響であることを示唆．
- 感想：言語生成の評価のための事前訓練として，多様な評価尺度に直接最適化するのが有効である，という点が面白かった．事前学習時に各タスクがどれくらい学習可能なのかという点は気になった（各事前学習タスクについて，5.4のablation studyにおける影響と，学習の易しさみたいなものが関係している気もした）．

# [Revisiting Grammatical Error Correction Evaluation and Beyond](https://aclanthology.org/2022.emnlp-main.463) (Gong et al., EMNLP 2022)

- 文法誤り訂正の参照あり評価尺度として，PT-M2を提案．従来のスパン一致ベースの枠組みと，BERTScoreなどのPretraining-based（PT-based）なスコアを組み合わせた．
- BERTScoreのようなPT-basedな尺度は多くのタスクで使われつつあるが，文法誤り訂正では使われていない．
  - 実際に使ってみると，文法誤り訂正特有の性質「多くの単語は変化しないこと」が原因で，スコアを正確に計算できないと主張．
- 提案法では，既存のスパンの一致に基づく参照あり評価において，スパンの重みをPT-basedな尺度で計算し，重み付きのprecision, recall, F0.5を考える．
  - 具体的には，入力文 $S$ ，参照文 $R$ ，入力に何らかの訂正ただ一つを加えた文 $S'$ を用いて $w=\mathrm{PTScore}(S', R) - \mathrm{PTScore}(S, R)$ で計算する． $\mathrm{PTScore}(\cdot)$ にはBERTScoreなどを利用できる．
  - $w$ は絶対値をとる．（以下は自分の意見）一見，訂正が悪いことを示す $w<0$ の場合にも，絶対値を取ることで正の評価を与えてしまうように思える．しかし，正解データに含まれる訂正で $w<0$ になる訂正は非常に少ないと考えられるため，実用上問題ないと思う．（モデルが $w$ が大きく負になるような悪い訂正をしても，そのような訂正は通常正解データにないのでPrecisionの分母がすごく大きくなるだけ）
- 実験ではCoNLL-14の参加システムの人手ランキング結果との相関について，提案法が従来の参照あり尺度およびPT-basedな尺度を上回ることを報告．
- 分析："良いこと"を判別する能力に長ける．"悪いこと"を評価するのは少し難あり．PT-basedな尺度のためのエンコーダのモデルサイズは，Base相当で十分で，特に大きいものを使う必要はない．
- 感想：ありそうでなかった評価尺度で，参照ありの尺度でありながらも人手評価との高く相関することが魅力．一方，分析はほぼ相関が高い・低いの話をしているが，もう少し他の観点からの話題も欲しい．例えば，どのようなことを評価するために使えるのか，評価結果からどのような分析につながるか，といったユースケースの点．もしくは，なぜ提案法は人手評価と高く相関するのか，という点も考察を与えて欲しい．多分，PT-basedな手法は内容語に敏感に反応して機能語はそんなに，みたいな傾向がある気がしていて，その傾向と人間が訂正結果の質を判断するときの傾向が似ている，みたいな話がある気がする．

# [Is this the end of the gold standard? A straightforward reference-less grammatical error correction metric](https://aclanthology.org/2021.emnlp-main.239) (Islam & Magnani, EMNLP 2021)

- 文法誤り訂正のための参照なし評価尺度としてScribendi Scoreを提案．

- 提案法の入力は誤り文(src)と訂正文(pred)．出力は訂正文の評価値であり，-1, 0, 1のいずれかの整数値．

- 評価のアルゴリズムは2段階．論文にもある擬似コードが分かりやすい．

  - perplexityの計算は適当な言語モデルを使う
  - token sort ratioは，単語を入れ替える訂正がある場合に正しく意味保存性を測定する狙い．
  - levenshtein distance ratioは文字レベルの編集距離を使う．この値がある程度高ければ，srcの意味が保持されていると思うことにする．

  ```python
  def Scribendi_score(src, pred):
  	if pred == src:
  		return 0 # 同じなら0
  	if perplexity(src) <= perplexity(pred):
  		return -1 # perplexityについてpredの方が悪ければ-1
  	else:
      trs = token_sort_ratio(src, pred)
      ldr = lev_dist_ratio(src, pred) # 文字レベル編集距離
      if max(trs, ldr) >= 0.8:
        return 1
      else:
        return -1
  ```

- 実験では，CoNLL-14の参加システムの人手ランキング結果と高く相関することを確認．参照なし評価尺度の既存手法であるSOMEには若干劣るが，SOMEと異なり提案法では特別な人手評価データを必要としないのが利点．
- また，提案法は同じ単語の繰り返しになっているpredictionをちゃんと"悪い"と評価できる．SOMEはそのようなpredictionについて高い評価を与えてしまう．
- 感想：人手評価との相関が高く，実装も簡単なので使いやすい尺度だと感じた．評価のアルゴリズムは，まず流暢性で足切りし，その後意味保存性を見ているともみなせる．-1が返るタイミングは2回あるため，どちらのタイミングかまで見ることで，システムの課題が流暢性もしくは意味保存性のどちらにあるのか分析可能かもしれない．

# [Grammatical Error Correction with Contrastive Learning in Low Error Density Domains](https://aclanthology.org/2021.findings-emnlp.419) (Cao et al., Findings 2021)

- 教師ありの文法誤り訂正において，Low Error Density Domain（含まれる誤りが少ないドメイン）のための対照学習手法を提案．
- 提案法は2段階の学習．1段階目はBEA-trainを用いた通常ドメインの学習，2段階目でLow Error Density Domainデータを用いた対照学習．
- 対照学習における目的関数は，通常の自己回帰モデルの損失関数 $L_{NLL}$ と，対照学習の提案損失関数 $L_{CL} = \max (-\log P(\mathrm{positiveサンプルの}y\mid x) + \log P(\mathrm{negativeサンプルの}y\mid x), 0))$ の足し算の形．
- positive sampleには訓練データ $<s_i, t_i>$ を使う．negative sampleには次の2つの戦略を用いる．
  - $\mathrm{CL}^-$： 1段階目の学習結果の重みによるk-best出力でターゲットを置き換えたものをnegativeに
  - $\mathrm{CL}$： $\mathrm{CL}^-$ に加えて $s_i \neq t_i$ の時に $<s_i, s_i>$ をnegativeに
- 実験では，ネイティブドメイン（誤りの数が少ない）の評価データCWEBで実験し，性能向上を確認．
- 分析では，negative samplingにおける2つ目の戦略（ $<s_i, s_i>$ ）がprecisionとrecallの両方を向上させる効果を持つ．また，対照学習によって過剰な訂正と不十分な訂正の両方を低減できる．
- 感想：Contrastive learningの枠組みを文法誤り訂正に適用したのは新しい．一方，研究目的はLow Error Density Domainにおける訂正性能を向上させることだが，ドメイン関係なく単に訂正性能が向上しただけのようにも見える．CWEBとCWEB以外の（writerが言語学習者であるような）評価データで性能の変化を比較した時に，CWEBで特に向上するかどうか，を見る必要があるのでは．

# [Controlling Grammatical Error Correction Using Word Edit Rate](https://aclanthology.org/P19-2020) (Hotate et al., ACL 2019)

- 教師ありの文法誤り訂正において，推論時に訂正の度合いを制御できるような学習手法を提案．
- 提案法では，source-target間の単語レベル編集率（WER, Word Edit Rate）に基づいて<1>（訂正が保守的である）から<5>（訂正が積極的である）までの記号をsourceの先頭に付与する．そのデータを用いて通常の自己回帰モデルの枠組みで学習する．
- 推論時には，所望の訂正の度合いに応じて<1>から<5>までのいずれかの記号をsourceに付与して入力する．

- 実験では，WERの記号によって実際に訂正の度合いを制御できることを確認．また，同じ入力でも<1>から<4>に変化させることで，高Precision低Recallから低Precision高Recallになり，PrecisionとRecallの重要度の観点からも制御可能．<5>は訓練データにノイズが多く，うまくいかない．
- ある入力文に<1>から<5>の記号を順に付与して生成することにより多様な5-bestの訂正文が得られる．これらのうちオラクルWERで評価すると，ベースラインを大幅に上回る．実際には，そのようなオラクルをリランキングで持ってくるのは難しい．また，意図的に評価データの文対から計算したWER（Gold WER）はオラクルWERとは異なっていて性能が出ない．
- 提案法により訓練されたモデルは，推論時に与えたWERよりも少ない度合いで訂正する傾向がある．
- 感想：先頭に特殊記号を付与するシンプルな手法なので使いやすそう．やはり適切なWERを推定するのは難しいという問題はあるので，Encoder側でWERの記号を推定させるマルチタスクで解いてみるのも面白そう．

# [Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding](https://aclanthology.org/2021.acl-long.462) (Sun et al., ACL-IJCNLP 2021)

- 教師ありの文法誤り訂正モデルとして，自己回帰モデルと非自己回帰モデルを組み合わせたShallow Aggressive Decoding (SAD) を提案．
- 提案法は非自己回帰→自己回帰→非自己回帰→....のように2種類の生成方法を交互に適用して訂正文を生成．入力のコピーとなる生成を非自己回帰が，書き換えとなる生成を自己回帰が担う．
  - 初回の非自己回帰による生成で入力と同じ文が生成されたら誤りがないと判断し終了．そうでない時，先頭から見て最初に入力文とは異なる部分を見つけて，そこからは自己回帰で生成．自己回帰で生成する中で再度入力と同じ系列を生成するようになれば，その先を非自己回帰で生成...を繰り返し適用．
- また，速度の向上を目的としてデコーダの層を浅くすることも実施．予備実験によりエンコーダを9層，デコーダを3層にするのが高性能（速度も1.8倍）．
- 英語での実験では，提案法によって速度はベースラインの10倍，訂正性能も向上．中国語でも実験を行い，性能を維持したまま速度が12倍．以上から，言語非依存で効果がある手法であることを主張．
- 速度について，より大きな編集が必要な文であるほど，自己回帰の仕事が多くなるため遅くなる．
- 感想：コピーできるところは一気に生成してしまうことで速度を上げているのが面白いし，目的と手法がちゃんと一致しているように感じた．気になる点があるとすれば，書き換えが必要なところは自己回帰モデルに完全に任せる手法だが，実際には非自己回帰モデルも何かしら生成しているわけで，その結果もうまく使えないかな，とは思った．その辺り，自己回帰モデルと非自己回帰モデルの訂正傾向の違いが分かれば，両者の出力をうまく使う方法論が確立できるかもしれない．

# [Type-Driven Multi-Turn Corrections for Grammatical Error Correction](https://aclanthology.org/2022.findings-acl.254) (Lai et al., Findings 2022)

- 系列タグ付けベースの教師あり文法誤り訂正において，誤りの依存関係を考慮した訂正，および訓練時と推論時のギャップを埋めるためにType-Driven Multi-Turn Corrections (TMTC)を提案．
  - 訓練時には一度のみ訂正するように学習するが，推論時には複数回の反復的な訂正を行うというギャップ．
- 提案法では既存の訓練データに含まれる誤り文 $X_e$ と訂正文 $X_c$ のペアに対する中間文 $X'$ を考え，「 $X_e → X'$ 」および「 $X'→X_c$ 」のように段階的な訂正を行うデータを構築する． $X'$ は，挿入・置換・削除の種類のうちいずれかの種類のみを手動で適用することで作成．
- ただし，「 $X_e→X'$ 」の学習においては，挿入・置換・削除のうち2種類の誤りは訂正されずに残るため，モデルが多くの誤りを訂正しないように学習してしまう．これを防ぐため， $X_e→X'$ のためのタグと，元の訓練データである $X_e→X_c$ のためのタグが一致しているようなトークンにのみ損失を流す（論文中の式3）．
- 実験では，提案法をGECToRに適用し，性能が向上することを確認．
- 分析:
  - $X'$ を作るために選んだ編集操作（挿入・置換・削除）が，一度目の訂正時により多く出力されることを確認（データの作り方がちゃんと学習に影響を与えている）．
  - $X'$ の時点で適用されている誤りが多いほど低Precission高Recallになる．（学習時のKEEPタグが減るから？）
  - 中間文を増やすことも考えられる（ $X_e→X'→X''→X_c$ みたいな）が，増やしても特に性能はほぼ伸びないか低下するので，1つで十分．

- 感想：学習時と推論時のギャップについては同じことを思っていたので参考になった．気になるのは $X'$ を作成するときに編集操作でまとめて適用している点で，本当にこれで依存関係やギャップの問題が解決するのかは疑問．例えば，同じ隙間に複数トークンを追加するときは挿入操作を反復的にやる必要があり， $X'$ には挿入途中の状況を再現するべき．一方，提案法ではそのような $X'$ は作れない．

# [Competence-based Curriculum Learning for Neural Machine Translation](https://aclanthology.org/N19-1119) (Platanios et al., NAACL 2019)

-  教師あり機械翻訳のためのカリキュラム学習手法としてCompetence-based Curriculum Learningを提案．
- 提案法は，DifficultyとCompetenceの2つのパラメータを主軸とする． 
  - Difficultyは，何らかの方法で訓練データの各サンプルの難易度を決めて，難易度の昇順でソートし，その累積密度関数上の値として定義する．
  - Competenceは，学習のタイムステップ $t$ の関数として定義する（学習率のスケジューラみたいなイメージ）．
  - DifficultyとCompetenceは共に，値域を[0,1]となるようにする．学習の流れとしては，ステップが進むにつれてCompetenceを更新し（増加させ），Competence以下のDifficultyである事例からミニバッチをサンプルする．
  - 例えば，5つの学習データのDifficultyが`[1,1,4,4,5]`だとすると， そのDifficultyは`[0.06, 0.13, 0.40, 0.66, 1]`となる．またタイムステップ $1\leq t \leq 5$ の範囲で，Competenceを雑に $competence(t)=0.2t$ とかに設定するならば，最初のタイムステップには`0.2`以下のdifficultyのサンプル（先頭から2つ分）が使われ，次には`0.4`以下のdifficultyのサンプル（先頭から3つ分）が使われ...，という具合．
- 各サンプルに対するDifficultyの定義には，単語のレア度や文長のどちらかを使用．（レアな単語が使われているほど難しい / 長い文であるほど難しい）
- Competenceの関数はいくつか定義しているが，実験結果で最も性能が良かったのは式8の $c_{sqrt} (t) \triangleq \min(1, \sqrt[2]{t\frac{1-c_0^2}{T} + c_0^2})$ ． $c_0$ はcompetenceの初期値で， $T$ は最大タイムステップ数．
- 実験では，WMT16 en-deなどの翻訳データについて，RNNおよびTransformerのモデルで実験．提案法により，BLEUスコアが2ポイント程度向上すること，学習速度が70%程度速くなることを確認．RNNよりもTransformerの方が効果が高い．
- 上の実験においてTransformerは学習率のスケジューラを用いていないのにも関わらず高性能なため，提案法が学習率スケジューラを代替したのでは，という仮説のもとで追加実験．実際に提案法はスケジューラを導入するよりも短い学習時間で性能向上に大きく寄与することを確認．
- 感想：提案法はアーキテクチャ非依存なため，任意のアーキテクチャに適用可能．また，難易度もタスクに応じて自由に決めていいので，翻訳以外のタスクにも容易に適用できるのが嬉しい．提案法のように連続的にデータを増やしていくカリキュラム学習では，特に難しい事例がミニバッチにサンプルされる機会が著しく少なくなる問題がある気はするが，そこは案外問題ないのだろうか．

# [Edit Distance Based Curriculum Learning for Paraphrase Generation](https://aclanthology.org/2021.acl-srw.24) (Kadotani et al., ACL-IJCNLP 2021)

- 教師あり言い換え生成のための，source-target間の編集距離に基づいたカリキュラム学習手法を提案．
- 編集距離が短いほど簡単であるとする．言い換えは少量の編集で済むものから大きく書き換えを要するものまで存在する．編集の度合いが小さい事例ほど簡単であるという直感から編集距離を用いる．

- 提案法では，Competence-basedのカリキュラム学習手法[[参考]](https://gotutiyan.github.io/paper-reading/#:~:text=Competence%2Dbased%20Curriculum%20Learning%20for%20Neural%20Machine%20Translation)に従う． 難易度にsource-target間の編集距離を使う．
- 実験では，単語のレア度もしくは文長を難易度としたベースラインと比較して，編集距離に基づく提案法が高い性能を達成することを示した．
- 分析では，評価データのサンプルを難易度の定義（レア度・文長・編集距離のいずれか）に基づいてEasy / Medium / Difficultに3分割し，各分割における性能を分析．提案法は特にDifficultの分割についてうまく処理できることを確認．
- 感想：編集距離を難易度とすることは，文法誤り訂正や平易化といった他の編集する系のタスクでも有効そう．カリキュラム学習の利点として学習速度の向上はよく言われる気はするので，言い換え生成タスクでも速度面で利点があるのかは気になる．

# [Pre-training a BERT with Curriculum Learning by Increasing Block-Size of Input Text](https://aclanthology.org/2021.ranlp-1.112) (Nagatsuka et al., RANLP 2021)

- BERTの事前学習において文長に基づくカリキュラム学習の有効性を検証．
  - self-attentionを計算するときに，短い文だとトークン間の関係が学習しやすく，長い文だと学習しにくいという直感による．

- 提案法では学習データの各事例から，64，128，256，512のトークンを持つ4種類の"ブロック"を作成する．ここでのトークンはbyte-level BPEに基づく分割結果．
- 短いブロックからデータを順に独立にモデルに与えていくことで4段階で事前学習する（データを加えていくのではないことに注意）．短いものから順に与えることでカリキュラム学習を実現する．
- 事前学習の実験では，提案法により収束の速度が向上することを確認． モデルのアーキテクチャにはRoBERTa-base相当を使う．
- 事前学習したモデルをGLUEでfinetuneして評価．ベースライン（いきなり512トークンのみで学習する場合）と比較して，提案法の方が多くのタスクで高性能．
- 分析
  - アンチカリキュラム学習（長い文から順に与える）と比較すると，GLUEの性能は「アンチカリキュラム＜カリキュラム」「アンチカリキュラム＞ベースライン」の傾向．特に後者の理由について，提案法のデータ作成法がデータ増強の効果を持つと考察．
  - いくつかの段階をスキップするカリキュラム学習（64→256→512など）と比較すると，4段階きっちりやった方が良い性能を示す傾向．ただし，比較結果は後段タスクによって異なることも確認．
- 感想：カリキュラム学習が事前学習時にも効果があるという結果で，参考になった．一方，ブロックの作り方が不明瞭な印象を受けた．長い文を短く切って64とか128のブロックを作ることは分かるが，論文に「ブロックはピリオドで終わるとは限らず，文の先頭単語で始まるとも限らない」みたいな記述があって，後者はマズいのではと感じた（文頭の正しさは保証しないと，positional embeddingの意味がなくなる）．

# [How Good (really) are Grammatical Error Correction Systems?](https://aclanthology.org/2021.eacl-main.231) (Rozovskaya & Roth, EACL 2021)

- 文法誤り訂正システムがどれほど優れているのかについて，新たに評価データを作ることで分析した論文．
- 既存の評価データでは文法誤り訂正システムの十分な評価ができていないと主張．この課題の解決のため，（入力文ではなく）システム訂正文を人手で訂正した新たな正解訂正文（CG, closest gold）を作成し，既存の評価データの正解訂正文（RG, reference gold）と比較した．
  - CGは，入力文に対する正解の訂正文の集合を仮定したとき，その中の最もシステム訂正文に近い要素であるとみなせる．
- 実験では英語とロシア語の評価データを用いている．各データセットから100文対をサンプルし，入力文に対する1-,2-,5-,10-bestの4種類のシステム訂正文についてCGをそれぞれ作成する．
- 知見1：RGを用いた評価では，システム性能は過小評価されている．
  - beam-searchにおける下位の訂正文であっても，実はまともな訂正ができている．RGの評価値は1-,2-,5-,10-bestになるにつれて低下するが，CGの評価値はほぼ一定である．また，下位の訂正文の方がより多く書き換える傾向にあり，いわゆる最小限の訂正に限らない訂正を行っている可能性がある（かつ，そうした訂正は評価データに現れにくいためRGの下では過小評価される）．
  - 翻訳分野における翻訳文の品質推定手法として，翻訳文を正しくするための書き換え度合いに注目するものがある（度合いが小さいほど品質が高い）．これを応用し，システム訂正文とCGの間の編集数を見る．結果は上位の訂正文でも下位の訂正文でもその編集数に大きな違いはなく，やはり下位の訂正文も十分な品質であることを示している．
- 知見2：これまでの1-bestによる文法誤り訂正システムの評価では，語彙的な訂正が過小評価されている可能性がある．まず誤りタイプを語彙的な訂正と文法的な訂正に分類し，それらの訂正の割合を1-bestと10-bestで比較．10-bestの方が語彙的な訂正が増加していた．

- 感想：やはり参照あり評価では限界があるということで，重要な課題だと感じる．人手評価との相関が高い参照なし評価が，CGを用いたような評価ができているのかは気になる．beam-searchにおいて下位の訂正文ほど語彙的な訂正が増えるというのは，個人的にかなり面白かった．システムの使い方の話としては，いわゆるFluency editを求めたい場合には意図的に下位の訂正文を使う方がよい，といったユースケースは考えられるかもしれない（とはいえ，どうやってそれをリランキングするのかという話はある）．

# [Position Offset Label Prediction for Grammatical Error Correction](https://aclanthology.org/2022.coling-1.480) (Wu et al., COLING 2022)

- 系列変換モデルに基づく教師あり文法誤り訂正において，「通常のトークン生成」と「訂正することでコピーするトークンがいくつ分ズレるか」をマルチタスクとして学習するモデルを提案．

- 提案法では，エンコーダ側に新たなタスクを追加する．具体的には，誤り文からみて訂正文の位置がどれくらいズレるか（=position offset）を分類問題として予測する．例えば， *I went park yesterday.* → *I went to the park yesterday.*のような訂正を考えると，positon offset labelは`[0 (I), 0 (went), 2 (park), 2 (yesterday), 2 (.)]`のようになり， *to the* が挿入されることでそれ以降のトークンが全て右に2つズレることを予測する（左にズレたら負の値になる）．

  - 先行研究でもトークンをコピーするかどうかをエンコーダで学習するモデルは存在するが，ラベルはコピーするかどうかの二値として定義されている．実際には多くのトークンはコピーされるので，これではラベルが不均衡になる．提案法では「コピーする」を示すラベルをoffsetに応じて細分化していると考えられ，不均衡の問題が緩和される．

- 推論時には，デコーダの通常のトークン推定確率と，エンコーダから得られるコピー確率の重み付き和を取った確率（論文の式12:  $P_v = (1-a_t)P_t^{gen} + a_t P_t^{copy}$ ）の下で推定する．ただし，エンコーダ側はあくまでもposition offsetを予測するため，コピー確率は次式（論文の式7）で求める．

   $$P_t^{copy} (y_t) = \left\{\begin{array}{ll} \Sigma_{i:x_i=y_t} P_o (o_i = t-i), & y_t \in X \\ 0 & otherwise \end{array} \right.$$ 
  - 特に上の式の意図は，訂正文の位置 $t$ にトークン $y_t$ を予測したくて，かつ $y_t$ と同じトークンが入力文 $X$ の位置 $i$ のトークンと同じである状況 ( $i:x_i=y_t)$ を考えたとき， 位置 $i$ から位置 $t$ にズレるようなposition offset label ( $o_i = t-i$ ) が $x_i$ にどれくらい高い確率で推定されているか？ということをコピー確率として捉えている．

- 上記のように，両者のタスクは推論時はパイプライン的な運用になるが，学習時は独立に学習される．

- 実験および分析では，
  - 提案法が言語非依存で効果がある手法であることを確認．英語よりも中国語と日本語により有効である傾向．
  - 提案法は様々なEncoderDecoderなアーキテクチャに導入でき，実際にどんなアーキテクチャに導入しても効果がありそう．
  - position offset label推定タスクは，通常のトークン推定の半分くらいの影響度で最適化するのが良い（ $Loss= Loss_{gen} + 0.5 Loss_{off}$ ）の重みによる最適化がベスト性能）．
- 感想：コピーラベルの細分化という観点から既存研究を拡張した研究で，面白かった．推論時にはパイプライン的なデータの流れになるため，position offset labelを正しく推定できることが重要だと思う．一方，Table 7を見るにその推定精度はPrecisionが0.35，Recallが0.17程度であるため，offset推定はかなり難しそうに見える．この理由は，おそらくトークンごとに独立にoffsetを推定しているので，全体で整合性を取るのが難しいから．トークンという粒度はかなり細かいので，スパンレベルでoffsetを推定するなどすれば安定しそうな気もする．それから，Table 5のCoNLL-14の結果において，ベースラインの性能が妙に低く見える．擬似誤りデータを使っているので，単純なTransformerでもF0.5=55.XXくらいは出そうな気がするが...．

# [IMPARA: Impact-Based Metric for GEC Using Parallel Data](https://aclanthology.org/2022.coling-1.316) (Maeda et al., COLING 2022)

- 文法誤り訂正の参照なし評価尺度としてIMPARAを提案．

- 提案法は，誤り文と訂正文の意味類似性を計算するモジュールと，訂正文の品質推定モジュールの2つのモジュールからなる．推論時は，基本的に品質推定の結果をそのまま評価値とするが，意味類似性による足切りがある．

  $score (X, Y) = \left\{\begin{array}{ll} Quality(Y) & (Similarity(X, Y) > threshold) \\ 0 & (otherwise) \end{array} \right.$

  - $Similarity(\cdot)$ には，単純にBERTの文ベクトルのコサイン類似度を用いる．

- 品質推定器は独自の方法で学習する．この推定器の入力は訂正文のみであり，その訂正文の品質をスカラーで出力する．定式化としてはBERTの[CLS]のベクトルを線形層に通して回帰として解く．目的関数は単体の訂正が意味保存性に与える影響度（Impact）の差分を学習できるように設計する．

  - Impactは，誤り文 $S$ と訂正文 $T$ の文ペアから抽出できる訂正の集合 $E$ があった時，その集合のうちただ一つの訂正 $e \in E$ を $T$ から除外した訂正文 $T_{-e}$ を考える．この時， $1- Similarity(T, T_{-e})$ のように計算する値は， $e$ を適用しないことで文意がどれくらい変わるかを示す指標であり，これをImpactと定義している．この時の $Similarity(\cdot)$ も，BERTの文ベクトルのコサイン類似度に基づく．

- 実験ではCoNLL-2014における人手評価相関と，MAEGEによるメタ評価結果の2種類からIMPARAの質を評価している．いずれも既存の参照なし評価手法のSOMEと同等か上回る性能．SOMEはその学習に専用の人手評価結果が必要だが，IMPARAはパラレルデータのみから学習可能であり，データ面で大きな利点がある（ドメイン適応も比較的容易．）．

- 分析では，内容語に関する訂正のImpactの方が機能語のImpactよりも大きくなる傾向を確認．また，文が長いほどImpactも小さくなる傾向がある．

- 感想：パラレルデータのみから評価機を構築できる点が嬉しい．論文では品質推定器の学習データのサンプル数を3200程度にfixしているが，もっと増やした場合にどうなるかは気になる（訓練データも使っていいはず？）．人手評価との相関が高い理由は，IMPARAが内容語に関する訂正を高く評価する尺度であって，人間もそれと同じ傾向で評価しているから，とかなのだろうか（[PT-M2](https://gotutiyan.github.io/paper-reading/#:~:text=Revisiting%20Grammatical%20Error%20Correction%20Evaluation%20and%20Beyond)でも同じようなことを書いた）

# [Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation](https://aclanthology.org/2023.findings-acl.779) (Mosbach et al., Findings 2023)
- Fine-tuning (FT)とIn-context learning (ICL)をフェアな条件で比較し，FTの方が良いことを示した．

- 言語モデルをタスクやドメインに適応させるための代表的な手段で，これまでの研究ではICLの方が良いと報告する先行研究があった．一方，先行研究ではFTとICLで異なるサイズのモデルを使用しており，本当にICLの手法そのものが効いているのかは分からない．（事前学習のデータが違うということもあるし）

- 提案法では，FTとICLをフェアな条件で比較する．実験には言語モデルにOPTを用いて，タスクを文分類（Natural language inferenceとparaphrase idetification）とする．OPTは多様なモデルサイズが提供されているためモデルサイズによる分析と，in-domainとout-of-domainそれぞれの結果の分析をする．

- 結果，in-domain設定ではICLとFTはほぼ同じ性能，out-of-domain (OOD)設定ではFTの方が高い性能を達成．

  - FTの設定についてもう少し細かく実験している．まずOODのためのcheckpointの選定について，大きなモデルではOODのvalidationを元に選定するべきで，小さなモデルではin-domain性能で選ぶと良い．次にFTに使うデータの数は，大きいモデルでは数に応じて性能は上がるが，小さなモデルではそうでもない．次にOODのvalidation setのサイズも50程度と小さくしても，OODのための適切なcheckpointを選べる．ただしseedによる性能のブレが大きい．この研究でのFTは基本full-finetuneで，かつ入力形式にはPattern-based Fine Tuning (PBFT)という手法を使っているが，full-finetuneでなくLoRAでも性能は維持され，PBFTはかなり有効．

- 最後にメタ的な観点からFTとICLを比較している．基本Table 3を見ておけば良いが，例えば以下のような話題がある．
  - FTは訓練事例を無制限に使えるが，ICLはコンテキスト長に制限される
  - FTは重みを更新するため再利用性が低いが，ICLは再利用可能．（FTでもLoRAなどで工夫すれば再利用性はグッと上がるが）
  - low-resourceな言語ではそもそも言語モデルの構築が難しいため，ICLはworkしない可能性が高い

- 感想：フェアな条件で比較し，結果として先行研究の言い分とは逆の結果になっている点がfindingらしくて良い．一方，タイトルがFT vs ICLと言っているものの，序盤で16-shotの結果を元に早々にFTが良いと結論づけられており，その後はFTの詳細分析に終始している感じもある．例えば16-shotだけではなく他の事例数も試して，事例数による差異は語れそうな気がする．Appendixにはそのような結果もあるが，bodyに持ってくるべきな気はする．それから，この論文の素直な受け取り方は「FTが強かった」だと思うが，個人的には「in-domainではFTもICLも似た結果」というのが実は重要にも見える．16-shot程度で十分なのであれば，目的のタスクやドメインに対して16個ぐらい手元でデータを作ればよく，そうすれば必ずin-domainな設定にできる．その上で，コストの高いFTをせずともICLで十分だ，ということは言えると思っており，これは実用を考えた時に嬉しい気がする．

# [Cloze Quality Estimation for Language Assessment](https://aclanthology.org/2023.findings-eacl.39) (Zhang et al., Findings 2023)

- Cloze testは，テキストの単語の穴埋め問題を選択肢から選ぶ形式で言語学習者の習熟度を測るテスト．このテストの質を測るためにCloze Quality Estimation（CQE）を提案．

- Cloze testの質を評価するにあたり，1. 複数の選択肢が妥当になってしまう，2.ある問題によって何の能力を測定可能か不明，の2つの問題を指摘．この解決のため，信頼性（1.の評価）と妥当性（2.の評価）の2つの軸で評価する枠組みを提案．
  - 信頼性は信頼できるかできないかの二値分類として評価．複数の選択肢が穴埋めの回答として適合してしまう場合に信頼できない問題と判断．
  - 妥当性は三値分類で，文法知識を測る問題，読解力を測る問題，妥当でない，として評価．

- データセットとしてCELAを提案．CELAはcloze testの問題とその質の評価結果を含む．

  - 問題のデータ：中国の高校の試験問題からground-truthなデータを持ってくる．これに加えて，正解以外の選択肢（distractor）を自動で生成する手法を用いて選択肢を用意したものも加える．これに加えて，正解の選択肢は固定し，それ以外の選択肢を自動生成した選択肢も加える．生成手法にはランダムサンプリングやPOSタグを考慮したものなど，計四種類を採用．つまり，一つの同じcloze testについて五種類の選択肢を含む（例えば4択問題のcloze testなら4択の候補が5種類ある）．

  - アノテーション：生成した問題についてその質をアノテーションする．一つの問題について3人がアノテーションし，3人の結果が一致したものだけ採用する．Kappa値は信頼性が0.67，妥当性が0.45．妥当性について，前置詞に関する問題が文法知識を問うものか読解力を問うものかで意見が割れる例があった．ラベルの分布はTable 3を参照．

- CELAには人手の評価結果が含まれるため，自動評価尺度の結果と比べることで評価器の質を評価できる．自動評価尺度の基本的な方針は，全ての選択肢を順番に穴埋めしていき，正解の選択肢と比べて偽の選択肢が文法と文意のどちらを惑わす目的なのかを判断する．それらの結果からルールベースに信頼性と妥当性を推定する（詳細はappendixのArgorithm 1）．この研究では文法および文意を壊すかどうかを判断する手法として，ルールベースとDNNベースの二種類を提案．

  - ルールベースは，正解の選択肢と偽の選択肢のラベル情報を必要とする．文法については，選択肢を穴埋めしたときに選択肢に付与されるPOSタグを，正解のものと偽のもので比べて推定する．POSタグが同じなら文法を惑わす選択肢である．文意については，POSタグの代わりに同義語かどうかを判断し，同義語なら文意を惑わす選択肢である．

  - DNNベースは，ある選択肢を穴埋めした文を文法誤り訂正器に入力し，その選択肢の部分に誤り訂正があれば文法もしくは文意の観点で誤りがあるとみなせる．次に，穴埋め前後の文を訂正前後の文であるとみなして，ERRANTでタグ付けする．タグ付の結果がADJ, ADV, NOUN, VERBなら文意を惑わす選択肢，それ以外なら文法を惑わす選択肢であるとみなす．

  - 結果としてはDNNベースがベースラインを超える結果．ただし，信頼性に関してはほとんどの問題を信頼できると判断してしまう傾向にある．ERRANTのタグにOTHERというタグがあって，これは文意を惑わす選択肢として扱う方が良さげ．

- 感想：正解の選択肢に対して偽の選択肢がどういう役割を持つか，という部分をちゃんと評価しようという姿勢がかなり好きであった．文法誤り訂正畑の人間からすると，GECシステムの応用例としても面白かった．論文ではGrammatical Correctorとなっていて，特にGECToRを使っているが，検出だけできれば良いのでtoken-level error detectionモデルを素直に学習した方が良い，ということはありそう（論文ではGECToRの訂正結果を誤り検出ラベルに変換したのか，誤り検出層の結果を直接使ったのか分からなかったが）．